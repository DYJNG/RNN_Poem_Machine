{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写诗机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyjng/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet import init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立字索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.idx_to_word.append(word)\n",
    "            self.word_to_idx[word] = len(idx_to_word) - 1\n",
    "        return self.word_to_idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path)\n",
    "    \n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3571633\n",
      "帝京篇十首 李世民 秦川雄帝宅 函谷壮皇居 绮殿千寻起 离宫百雉馀 连薨遥接汉 飞观迥凌虚 云日隐层阙 风烟出绮疏 岩廊罢机务 崇文聊驻辇 玉匣启龙图 金绳披凤篆 韦编断仍续 缥帙舒还卷 对此乃淹留 欹案观坟典 移步出词林 停舆欣武宴 雕弓写明月 骏马疑流电 惊雁落虚弦 啼猿悲急箭 阅赏诚多美 于兹乃忘倦 鸣笳临乐馆 眺听欢芳节 急管韵朱弦 清歌凝白雪 彩凤肃来仪 玄鹤纷成列 去兹郑卫声 雅音方可悦 芳辰追逸趣 禁苑信多奇 桥形通汉上 峰势接云危 烟霞交隐映 花鸟自参差 何如肆辙迹 万里赏瑶池 飞盖去芳园 兰桡游翠渚 萍间日彩乱 荷处香风举 桂楫满中川 弦歌振长屿 岂必汾河曲 方为欢宴所 落日双阙昏 回舆九重暮 长烟散初碧 皎月澄轻素 搴幌玩琴书 开轩引云雾 斜汉耿层阁 清风摇玉树 欢乐难再逢 芳辰良可惜 玉酒泛云罍 兰殽陈绮席 千钟合尧禹 百兽谐金石 得志重寸阴 忘怀轻尺璧 建章欢赏夕 二八尽妖妍 罗绮昭阳殿 芬芳玳瑁筵 佩移星正动 扇掩月初圆 无劳上悬圃 即此对神仙 以兹游观极 悠然独长想 披卷览前踪 抚躬寻既往 望古茅茨约 瞻今兰殿广 人道恶高危 虚心戒盈荡 奉天竭诚敬 临民思惠养 纳善察忠谏 明科慎刑赏 六五诚难继 四三非易仰 广待淳化敷 方嗣云亭响 饮马长城窟行 李世民 塞外悲风切 交河冰已结 瀚海百重波 阴山千里雪 迥戍危烽火 层峦引高节 悠悠卷旆旌 饮马出长城 寒沙连骑迹 朔吹断边声 胡尘清玉塞 羌笛韵金钲 绝漠干戈戢 车徒振原隰 都尉反龙堆 将军旋马邑 扬麾氛雾静 纪石功名立 荒裔一戎衣 灵台凯歌入 执契静三边 李世民 执契静三边 持衡临万姓 玉彩辉关烛 金华流日镜 无为宇宙清 有美璇玑正 皎佩星连景 飘衣云结庆 戢武耀七德 升文辉九功 烟波澄旧碧 尘火息前红 霜野韬莲剑 关城罢月弓 钱缀榆天合 新城柳塞空 花销葱岭雪 縠尽流沙雾 秋驾转兢怀 春冰弥轸虑 书绝龙庭羽 烽休凤穴戍 衣宵寝二难 食旰餐三惧 翦暴兴先废 除凶存昔亡 圆盖归天壤 方舆入地荒 孔海池京邑 双河沼帝乡 循躬思励己 抚俗愧时康 元首伫盐梅 股肱惟辅弼 羽贤崆岭四 翼圣襄城七 浇俗庶反淳 替文聊就质 已知隆至道 共欢区宇一 正日临朝 李世民 条风开献节 灰律动初阳 百蛮奉遐赆 万国朝未央 虽无舜禹迹 幸欣天地康 车轨同八表 书文混四方 赫奕俨冠盖 纷纶盛服章 羽旄飞驰道 钟鼓震岩廊 \n"
     ]
    }
   ],
   "source": [
    "with open('./data/poem_restruct.txt') as f:\n",
    "#     corpus_chars = f.read()\n",
    "    corpus_chars = ''\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "#         char = ''\n",
    "        for i, word in enumerate(words):\n",
    "#             if len(word) < 5:\n",
    "#                 words[i] = ''\n",
    "#             corpus_char += words[i] \n",
    "#             corpus_char += ' '\n",
    "#             for j in range(len(word)):\n",
    "#                 corpus_chars += word[j]\n",
    "#             corpus_chars += ' '\n",
    "            corpus_chars += words[i]\n",
    "            corpus_chars += ' '\n",
    "    print(len(corpus_chars))\n",
    "    print(corpus_chars[0:1000])\n",
    "# char = char.replace('\\n', ' ').replace('\\r', ' ')\n",
    "# print(char[2])\n",
    "# print(len(char))\n",
    "#             texts = list(set(word))\n",
    "#             for text in texts:\n",
    "#                 self.dictionary.add_word(word)\n",
    "# len(corpus_chars)\n",
    "# corpus_chars[10000:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7625\n",
      "2097\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "print(vocab_size)\n",
    "print(char_to_idx[' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 帝京篇十首 李世民 秦川雄帝宅 函谷壮皇居 绮殿千寻起 离宫百雉馀 连薨遥接汉 飞观迥凌虚 云日隐层阙 风烟出绮疏 岩廊罢机务 崇文聊驻辇 玉匣启龙图 金绳披凤篆 韦编断仍续 缥帙舒还卷 对此乃淹留 \n",
      "\n",
      "indices: \n",
      " [5311, 7240, 5180, 1557, 6791, 2097, 5708, 3042, 6951, 2097, 5227, 7253, 1512, 5311, 3340, 2097, 4034, 3939, 5591, 6245, 1765, 2097, 4059, 3627, 5848, 6749, 1654, 2097, 3746, 5612, 6438, 2709, 1596, 2097, 5519, 5908, 4809, 2002, 7579, 2097, 4409, 533, 6676, 1996, 653, 2097, 3057, 4093, 1340, 1836, 686, 2097, 1915, 5811, 3990, 4059, 5878, 2097, 2425, 6552, 5648, 283, 6118, 2097, 958, 6662, 2827, 2085, 3953, 2097, 1924, 1952, 2123, 3499, 470, 2097, 611, 2321, 4132, 6402, 3282, 2097, 4483, 2255, 6620, 6337, 585, 2097, 5762, 7606, 5735, 4544, 502, 2097, 4312, 6495, 4018, 4232, 616, 2097]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:100]\n",
    "print('chars: \\n', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('\\nindices: \\n', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, mode, vocab_size, embed_size, num_hiddens, \n",
    "                 num_layers, drop_prob=0.5, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            # 将词索引变换成词向量。这些词向量也是模型参宿。\n",
    "            self.embedding = nn.Embedding(\n",
    "                vocab_size, embed_size, weight_initializer=init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hiddens, num_layers, activation='relu', \n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hiddens, num_layers, activation='tanh', \n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hiddens, num_layers, \n",
    "                                    dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hiddens, num_layers, \n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            else:\n",
    "                raise ValueError('Invalid mode %s. Options are rnn_relu, ' \n",
    "                                 'rnn_tanh, lstm, and gru' % mode)\n",
    "            self.dense = nn.Dense(vocab_size, in_units=num_hiddens)\n",
    "            self.num_hiddens = num_hiddens\n",
    "    \n",
    "    def forward(self, inputs, state):\n",
    "        embedding = self.dropout(self.embedding(inputs))\n",
    "        output, state = self.rnn(embedding, state)\n",
    "        output = self.dropout(output)\n",
    "        output = self.dense(output.reshape((-1, self.num_hiddens)))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode_name = 'lstm'\n",
    "embed_dim = 300\n",
    "hidden_dim = 300\n",
    "num_layers = 2\n",
    "lr = 1.0\n",
    "clipping_norm =0.2\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "num_steps = 15\n",
    "dropout_rate = 0.3\n",
    "eval_period = 500\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    # output shape : num_batches * batch_size\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches*batch_size]\n",
    "    data = data.reshape((batch_size, num_batches)).T\n",
    "    return data\n",
    "\n",
    "\n",
    "corpus_indices = nd.array(corpus_indices, dtype='int32')\n",
    "train_data = batchify(corpus_indices, batch_size).as_in_context(ctx)\n",
    "\n",
    "model = RNNModel(mode_name, vocab_size, embed_dim, hidden_dim, \n",
    "                 num_layers, dropout_rate)\n",
    "\n",
    "model.collect_params().initialize(init.Xavier(),ctx=ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', \n",
    "                        {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(num_steps, source.shape[0]-1-i)\n",
    "    data = source[i: i+seq_len]\n",
    "    target = source[i+1: i+1+seq_len]\n",
    "    return data, target.reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从计算图分离隐含状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detach(state):\n",
    "    if isinstance(state, (tuple, list)):\n",
    "        state = [i.detach() for i in state]\n",
    "    else:\n",
    "        state = state.detach()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func=nd.zeros, \n",
    "                                   batch_size=batch_size, ctx=ctx)\n",
    "        for batch_i, idx in enumerate(range(0, train_data.shape[0]-1, \n",
    "                                         num_steps)):\n",
    "            data, target = get_batch(train_data, idx)\n",
    "            # 从计算图分离隐含状态\n",
    "            hidden = detach(hidden)\n",
    "#             print(hidden[1].shape)\n",
    "#             print(data.shape)\n",
    "            with ag.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "            L.backward()\n",
    "            \n",
    "            grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "            gluon.utils.clip_global_norm(grads, \n",
    "                                         clipping_norm*num_steps*batch_size)\n",
    "            trainer.step(batch_size)\n",
    "            total_L += nd.sum(L).asscalar()\n",
    "            \n",
    "            if batch_i % eval_period == 0 and batch_i > 0:\n",
    "                cur_L = total_L / num_steps / batch_size / eval_period\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (epoch+1, batch_i, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0\n",
    "            \n",
    "        print('[Epoch %d] time_cost %.2fs' % (epoch+1, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 4.53, perplexity 92.62\n",
      "[Epoch 1 Batch 1000] loss 4.52, perplexity 91.95\n",
      "[Epoch 1 Batch 1500] loss 4.53, perplexity 93.12\n",
      "[Epoch 1] time_cost 46.24s\n",
      "[Epoch 2 Batch 500] loss 4.51, perplexity 91.01\n",
      "[Epoch 2 Batch 1000] loss 4.50, perplexity 90.45\n",
      "[Epoch 2 Batch 1500] loss 4.52, perplexity 91.59\n",
      "[Epoch 2] time_cost 46.09s\n",
      "[Epoch 3 Batch 500] loss 4.49, perplexity 89.48\n",
      "[Epoch 3 Batch 1000] loss 4.49, perplexity 89.03\n",
      "[Epoch 3 Batch 1500] loss 4.50, perplexity 90.30\n",
      "[Epoch 3] time_cost 46.13s\n",
      "[Epoch 4 Batch 500] loss 4.48, perplexity 88.37\n",
      "[Epoch 4 Batch 1000] loss 4.48, perplexity 88.03\n",
      "[Epoch 4 Batch 1500] loss 4.49, perplexity 89.14\n",
      "[Epoch 4] time_cost 46.15s\n",
      "[Epoch 5 Batch 500] loss 4.47, perplexity 87.31\n",
      "[Epoch 5 Batch 1000] loss 4.47, perplexity 86.97\n",
      "[Epoch 5 Batch 1500] loss 4.48, perplexity 88.27\n",
      "[Epoch 5] time_cost 46.10s\n",
      "[Epoch 6 Batch 500] loss 4.46, perplexity 86.34\n",
      "[Epoch 6 Batch 1000] loss 4.46, perplexity 86.18\n",
      "[Epoch 6 Batch 1500] loss 4.47, perplexity 87.56\n",
      "[Epoch 6] time_cost 46.16s\n",
      "[Epoch 7 Batch 500] loss 4.45, perplexity 85.61\n",
      "[Epoch 7 Batch 1000] loss 4.45, perplexity 85.24\n",
      "[Epoch 7 Batch 1500] loss 4.46, perplexity 86.70\n",
      "[Epoch 7] time_cost 46.18s\n",
      "[Epoch 8 Batch 500] loss 4.44, perplexity 84.76\n",
      "[Epoch 8 Batch 1000] loss 4.44, perplexity 84.64\n",
      "[Epoch 8 Batch 1500] loss 4.45, perplexity 85.82\n",
      "[Epoch 8] time_cost 46.24s\n",
      "[Epoch 9 Batch 500] loss 4.43, perplexity 84.07\n",
      "[Epoch 9 Batch 1000] loss 4.43, perplexity 84.04\n",
      "[Epoch 9 Batch 1500] loss 4.44, perplexity 85.19\n",
      "[Epoch 9] time_cost 46.20s\n",
      "[Epoch 10 Batch 500] loss 4.43, perplexity 83.54\n",
      "[Epoch 10 Batch 1000] loss 4.42, perplexity 83.37\n",
      "[Epoch 10 Batch 1500] loss 4.44, perplexity 84.62\n",
      "[Epoch 10] time_cost 46.64s\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_params('./rnn_params_epoch_20.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = RNNModel(mode_name, vocab_size, embed_dim, hidden_dim, \n",
    "                 num_layers, dropout_rate)\n",
    "model1.load_params('./rnn_params.txt', ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rnn(model, prefix, num_chars, ctx, idx_to_char, char_to_idx):\n",
    "    prefix = prefix.lower()  # 大写字符转化为小写\n",
    "    hidden = model1.begin_state(func=nd.zeros, \n",
    "                               batch_size=1, ctx=ctx)\n",
    "#     print(hidden[0].shape)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        data = nd.array([output[-1]], dtype='int32', ctx=ctx)\n",
    "        data = data.reshape((1,1))\n",
    "#         print(data.shape)\n",
    "        pred, hidden = model(data, hidden)\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = int(pred.argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 青丝 李白 玉女不知人 青山不可攀 一朝三五月 一片一枝花 春风吹白日 春色照青春 不见春风至 春风吹柳条\n"
     ]
    }
   ],
   "source": [
    "seq = '青丝'\n",
    "pred_len = 50\n",
    "print('---', predict_rnn(model1, seq, pred_len, ctx, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn.RNN??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = model1.begin_state(func=nd.zeros, \n",
    "                               batch_size=1, ctx=ctx)\n",
    "data = nd.array([char_to_idx['个']], dtype='int32', ctx=ctx)\n",
    "data = data.reshape((1,1))\n",
    "print(data)\n",
    "output, hidden = model1(data, hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(idx_to_char[int(output.argmax(axis=1).asscalar())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
