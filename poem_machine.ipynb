{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写诗机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyjng/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet import init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立字索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.idx_to_word.append(word)\n",
    "            self.word_to_idx[word] = len(idx_to_word) - 1\n",
    "        return self.word_to_idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path)\n",
    "    \n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3571633\n",
      "帝京篇十首 李世民 秦川雄帝宅 函谷壮皇居 绮殿千寻起 离宫百雉馀 连薨遥接汉 飞观迥凌虚 云日隐层阙 风烟出绮疏 岩廊罢机务 崇文聊驻辇 玉匣启龙图 金绳披凤篆 韦编断仍续 缥帙舒还卷 对此乃淹留 欹案观坟典 移步出词林 停舆欣武宴 雕弓写明月 骏马疑流电 惊雁落虚弦 啼猿悲急箭 阅赏诚多美 于兹乃忘倦 鸣笳临乐馆 眺听欢芳节 急管韵朱弦 清歌凝白雪 彩凤肃来仪 玄鹤纷成列 去兹郑卫声 雅音方可悦 芳辰追逸趣 禁苑信多奇 桥形通汉上 峰势接云危 烟霞交隐映 花鸟自参差 何如肆辙迹 万里赏瑶池 飞盖去芳园 兰桡游翠渚 萍间日彩乱 荷处香风举 桂楫满中川 弦歌振长屿 岂必汾河曲 方为欢宴所 落日双阙昏 回舆九重暮 长烟散初碧 皎月澄轻素 搴幌玩琴书 开轩引云雾 斜汉耿层阁 清风摇玉树 欢乐难再逢 芳辰良可惜 玉酒泛云罍 兰殽陈绮席 千钟合尧禹 百兽谐金石 得志重寸阴 忘怀轻尺璧 建章欢赏夕 二八尽妖妍 罗绮昭阳殿 芬芳玳瑁筵 佩移星正动 扇掩月初圆 无劳上悬圃 即此对神仙 以兹游观极 悠然独长想 披卷览前踪 抚躬寻既往 望古茅茨约 瞻今兰殿广 人道恶高危 虚心戒盈荡 奉天竭诚敬 临民思惠养 纳善察忠谏 明科慎刑赏 六五诚难继 四三非易仰 广待淳化敷 方嗣云亭响 饮马长城窟行 李世民 塞外悲风切 交河冰已结 瀚海百重波 阴山千里雪 迥戍危烽火 层峦引高节 悠悠卷旆旌 饮马出长城 寒沙连骑迹 朔吹断边声 胡尘清玉塞 羌笛韵金钲 绝漠干戈戢 车徒振原隰 都尉反龙堆 将军旋马邑 扬麾氛雾静 纪石功名立 荒裔一戎衣 灵台凯歌入 执契静三边 李世民 执契静三边 持衡临万姓 玉彩辉关烛 金华流日镜 无为宇宙清 有美璇玑正 皎佩星连景 飘衣云结庆 戢武耀七德 升文辉九功 烟波澄旧碧 尘火息前红 霜野韬莲剑 关城罢月弓 钱缀榆天合 新城柳塞空 花销葱岭雪 縠尽流沙雾 秋驾转兢怀 春冰弥轸虑 书绝龙庭羽 烽休凤穴戍 衣宵寝二难 食旰餐三惧 翦暴兴先废 除凶存昔亡 圆盖归天壤 方舆入地荒 孔海池京邑 双河沼帝乡 循躬思励己 抚俗愧时康 元首伫盐梅 股肱惟辅弼 羽贤崆岭四 翼圣襄城七 浇俗庶反淳 替文聊就质 已知隆至道 共欢区宇一 正日临朝 李世民 条风开献节 灰律动初阳 百蛮奉遐赆 万国朝未央 虽无舜禹迹 幸欣天地康 车轨同八表 书文混四方 赫奕俨冠盖 纷纶盛服章 羽旄飞驰道 钟鼓震岩廊 \n"
     ]
    }
   ],
   "source": [
    "with open('./data/poem_restruct.txt') as f:\n",
    "#     corpus_chars = f.read()\n",
    "    corpus_chars = ''\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "#         char = ''\n",
    "        for i, word in enumerate(words):\n",
    "#             if len(word) < 5:\n",
    "#                 words[i] = ''\n",
    "#             corpus_char += words[i] \n",
    "#             corpus_char += ' '\n",
    "#             for j in range(len(word)):\n",
    "#                 corpus_chars += word[j]\n",
    "#             corpus_chars += ' '\n",
    "            corpus_chars += words[i]\n",
    "            corpus_chars += ' '\n",
    "    print(len(corpus_chars))\n",
    "    print(corpus_chars[0:1000])\n",
    "# char = char.replace('\\n', ' ').replace('\\r', ' ')\n",
    "# print(char[2])\n",
    "# print(len(char))\n",
    "#             texts = list(set(word))\n",
    "#             for text in texts:\n",
    "#                 self.dictionary.add_word(word)\n",
    "# len(corpus_chars)\n",
    "# corpus_chars[10000:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7625\n",
      "6248\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "print(vocab_size)\n",
    "print(char_to_idx[' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 帝京篇十首 李世民 秦川雄帝宅 函谷壮皇居 绮殿千寻起 离宫百雉馀 连薨遥接汉 飞观迥凌虚 云日隐层阙 风烟出绮疏 岩廊罢机务 崇文聊驻辇 玉匣启龙图 金绳披凤篆 韦编断仍续 缥帙舒还卷 对此乃淹留 \n",
      "\n",
      "indices: \n",
      " [2689, 4150, 4053, 1090, 2855, 6248, 6597, 1822, 6615, 6248, 6558, 3075, 5632, 2689, 3929, 6248, 7619, 3188, 6918, 5260, 5728, 6248, 1741, 2460, 1317, 6633, 6071, 6248, 5579, 1081, 3654, 2838, 5102, 6248, 2012, 4253, 236, 4609, 3170, 6248, 7433, 386, 5583, 1632, 221, 6248, 1547, 3071, 1064, 833, 4252, 6248, 3583, 3175, 334, 1741, 1395, 6248, 3613, 5718, 369, 3145, 5755, 6248, 4046, 1749, 5420, 3471, 4527, 6248, 6372, 3852, 2656, 7023, 3923, 6248, 5715, 4697, 5488, 2008, 4333, 6248, 5456, 4879, 4761, 4690, 1597, 6248, 2236, 2728, 230, 1086, 4236, 6248, 1, 7544, 7384, 2119, 6551, 6248]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:100]\n",
    "print('chars: \\n', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('\\nindices: \\n', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, mode, vocab_size, embed_size, num_hiddens, \n",
    "                 num_layers, drop_prob=0.5, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            # 将词索引变换成词向量。这些词向量也是模型参宿。\n",
    "            self.embedding = nn.Embedding(\n",
    "                vocab_size, embed_size, weight_initializer=init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hiddens, num_layers, activation='relu', \n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hiddens, num_layers, activation='tanh', \n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hiddens, num_layers, \n",
    "                                    dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hiddens, num_layers, \n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            else:\n",
    "                raise ValueError('Invalid mode %s. Options are rnn_relu, ' \n",
    "                                 'rnn_tanh, lstm, and gru' % mode)\n",
    "            self.dense = nn.Dense(vocab_size, in_units=num_hiddens)\n",
    "            self.num_hiddens = num_hiddens\n",
    "    \n",
    "    def forward(self, inputs, state):\n",
    "        embedding = self.dropout(self.embedding(inputs))\n",
    "        output, state = self.rnn(embedding, state)\n",
    "        output = self.dropout(output)\n",
    "        output = self.dense(output.reshape((-1, self.num_hiddens)))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode_name = 'lstm'\n",
    "embed_dim = 300\n",
    "hidden_dim = 300\n",
    "num_layers = 2\n",
    "lr = 1\n",
    "clipping_norm =0.2\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "num_steps = 15\n",
    "dropout_rate = 0.3\n",
    "eval_period = 500\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    # output shape : num_batches * batch_size\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches*batch_size]\n",
    "    data = data.reshape((batch_size, num_batches)).T\n",
    "    return data\n",
    "\n",
    "\n",
    "corpus_indices = nd.array(corpus_indices, dtype='int32')\n",
    "train_data = batchify(corpus_indices, batch_size).as_in_context(ctx)\n",
    "\n",
    "model = RNNModel(mode_name, vocab_size, embed_dim, hidden_dim, \n",
    "                 num_layers, dropout_rate)\n",
    "\n",
    "model.collect_params().initialize(init.Xavier(),ctx=ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', \n",
    "                        {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(num_steps, source.shape[0]-1-i)\n",
    "    data = source[i: i+seq_len]\n",
    "    target = source[i+1: i+1+seq_len]\n",
    "    return data, target.reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从计算图分离隐含状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detach(state):\n",
    "    if isinstance(state, (tuple, list)):\n",
    "        state = [i.detach() for i in state]\n",
    "    else:\n",
    "        state = state.detach()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func=nd.zeros, \n",
    "                                   batch_size=batch_size, ctx=ctx)\n",
    "        if (epoch+1) % 15 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate*0.3)\n",
    "        for batch_i, idx in enumerate(range(0, train_data.shape[0]-1, \n",
    "                                         num_steps)):\n",
    "            data, target = get_batch(train_data, idx)\n",
    "            # 从计算图分离隐含状态\n",
    "            hidden = detach(hidden)\n",
    "#             print(hidden[1].shape)\n",
    "#             print(data.shape)\n",
    "            with ag.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "            L.backward()\n",
    "            \n",
    "            grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "            gluon.utils.clip_global_norm(grads, \n",
    "                                         clipping_norm*num_steps*batch_size)\n",
    "            trainer.step(batch_size)\n",
    "            total_L += nd.sum(L).asscalar()\n",
    "            \n",
    "            if batch_i % eval_period == 0 and batch_i > 0:\n",
    "                cur_L = total_L / num_steps / batch_size / eval_period\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (epoch+1, batch_i, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0\n",
    "            \n",
    "        print('[Epoch %d] time_cost %.2fs lr %f' % (epoch+1, time.time()-start_time, trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 6.74, perplexity 845.07\n",
      "[Epoch 1 Batch 1000] loss 6.05, perplexity 422.14\n",
      "[Epoch 1 Batch 1500] loss 5.86, perplexity 351.22\n",
      "[Epoch 1] time_cost 46.28s lr 1.000000\n",
      "[Epoch 2 Batch 500] loss 5.53, perplexity 251.25\n",
      "[Epoch 2 Batch 1000] loss 5.33, perplexity 206.22\n",
      "[Epoch 2 Batch 1500] loss 5.24, perplexity 188.36\n",
      "[Epoch 2] time_cost 46.06s lr 1.000000\n",
      "[Epoch 3 Batch 500] loss 5.09, perplexity 162.43\n",
      "[Epoch 3 Batch 1000] loss 5.02, perplexity 151.49\n",
      "[Epoch 3 Batch 1500] loss 4.99, perplexity 147.22\n",
      "[Epoch 3] time_cost 46.09s lr 1.000000\n",
      "[Epoch 4 Batch 500] loss 4.90, perplexity 134.92\n",
      "[Epoch 4 Batch 1000] loss 4.86, perplexity 129.39\n",
      "[Epoch 4 Batch 1500] loss 4.85, perplexity 127.91\n",
      "[Epoch 4] time_cost 46.17s lr 1.000000\n",
      "[Epoch 5 Batch 500] loss 4.79, perplexity 120.42\n",
      "[Epoch 5 Batch 1000] loss 4.76, perplexity 117.06\n",
      "[Epoch 5 Batch 1500] loss 4.76, perplexity 116.89\n",
      "[Epoch 5] time_cost 46.09s lr 1.000000\n",
      "[Epoch 6 Batch 500] loss 4.71, perplexity 111.53\n",
      "[Epoch 6 Batch 1000] loss 4.69, perplexity 109.37\n",
      "[Epoch 6 Batch 1500] loss 4.70, perplexity 109.70\n",
      "[Epoch 6] time_cost 46.09s lr 1.000000\n",
      "[Epoch 7 Batch 500] loss 4.66, perplexity 105.62\n",
      "[Epoch 7 Batch 1000] loss 4.64, perplexity 103.94\n",
      "[Epoch 7 Batch 1500] loss 4.65, perplexity 104.74\n",
      "[Epoch 7] time_cost 46.03s lr 1.000000\n",
      "[Epoch 8 Batch 500] loss 4.62, perplexity 101.36\n",
      "[Epoch 8 Batch 1000] loss 4.61, perplexity 100.08\n",
      "[Epoch 8 Batch 1500] loss 4.62, perplexity 101.02\n",
      "[Epoch 8] time_cost 46.05s lr 1.000000\n",
      "[Epoch 9 Batch 500] loss 4.59, perplexity 98.10\n",
      "[Epoch 9 Batch 1000] loss 4.57, perplexity 97.02\n",
      "[Epoch 9 Batch 1500] loss 4.59, perplexity 98.01\n",
      "[Epoch 9] time_cost 46.07s lr 1.000000\n",
      "[Epoch 10 Batch 500] loss 4.56, perplexity 95.63\n",
      "[Epoch 10 Batch 1000] loss 4.55, perplexity 94.69\n",
      "[Epoch 10 Batch 1500] loss 4.56, perplexity 95.70\n",
      "[Epoch 10] time_cost 46.07s lr 1.000000\n",
      "[Epoch 11 Batch 500] loss 4.54, perplexity 93.32\n",
      "[Epoch 11 Batch 1000] loss 4.53, perplexity 92.84\n",
      "[Epoch 11 Batch 1500] loss 4.54, perplexity 93.82\n",
      "[Epoch 11] time_cost 46.12s lr 1.000000\n",
      "[Epoch 12 Batch 500] loss 4.52, perplexity 91.76\n",
      "[Epoch 12 Batch 1000] loss 4.51, perplexity 91.28\n",
      "[Epoch 12 Batch 1500] loss 4.52, perplexity 92.27\n",
      "[Epoch 12] time_cost 46.09s lr 1.000000\n",
      "[Epoch 13 Batch 500] loss 4.50, perplexity 90.30\n",
      "[Epoch 13 Batch 1000] loss 4.50, perplexity 89.85\n",
      "[Epoch 13 Batch 1500] loss 4.51, perplexity 91.02\n",
      "[Epoch 13] time_cost 46.08s lr 1.000000\n",
      "[Epoch 14 Batch 500] loss 4.49, perplexity 88.96\n",
      "[Epoch 14 Batch 1000] loss 4.49, perplexity 88.74\n",
      "[Epoch 14 Batch 1500] loss 4.50, perplexity 89.64\n",
      "[Epoch 14] time_cost 46.06s lr 1.000000\n",
      "[Epoch 15 Batch 500] loss 4.48, perplexity 87.93\n",
      "[Epoch 15 Batch 1000] loss 4.47, perplexity 87.61\n",
      "[Epoch 15 Batch 1500] loss 4.48, perplexity 88.66\n",
      "[Epoch 15] time_cost 46.09s lr 1.000000\n",
      "[Epoch 16 Batch 500] loss 4.47, perplexity 87.07\n",
      "[Epoch 16 Batch 1000] loss 4.46, perplexity 86.61\n",
      "[Epoch 16 Batch 1500] loss 4.47, perplexity 87.69\n",
      "[Epoch 16] time_cost 46.08s lr 1.000000\n",
      "[Epoch 17 Batch 500] loss 4.46, perplexity 86.08\n",
      "[Epoch 17 Batch 1000] loss 4.45, perplexity 86.00\n",
      "[Epoch 17 Batch 1500] loss 4.47, perplexity 86.93\n",
      "[Epoch 17] time_cost 46.15s lr 1.000000\n",
      "[Epoch 18 Batch 500] loss 4.45, perplexity 85.51\n",
      "[Epoch 18 Batch 1000] loss 4.45, perplexity 85.21\n",
      "[Epoch 18 Batch 1500] loss 4.46, perplexity 86.28\n",
      "[Epoch 18] time_cost 46.11s lr 1.000000\n",
      "[Epoch 19 Batch 500] loss 4.44, perplexity 84.72\n",
      "[Epoch 19 Batch 1000] loss 4.44, perplexity 84.55\n",
      "[Epoch 19 Batch 1500] loss 4.45, perplexity 85.64\n",
      "[Epoch 19] time_cost 46.12s lr 1.000000\n",
      "[Epoch 20 Batch 500] loss 4.43, perplexity 84.17\n",
      "[Epoch 20 Batch 1000] loss 4.43, perplexity 84.03\n",
      "[Epoch 20 Batch 1500] loss 4.44, perplexity 84.97\n",
      "[Epoch 20] time_cost 46.12s lr 1.000000\n",
      "[Epoch 21 Batch 500] loss 4.43, perplexity 83.52\n",
      "[Epoch 21 Batch 1000] loss 4.42, perplexity 83.45\n",
      "[Epoch 21 Batch 1500] loss 4.44, perplexity 84.59\n",
      "[Epoch 21] time_cost 46.13s lr 1.000000\n",
      "[Epoch 22 Batch 500] loss 4.42, perplexity 83.04\n",
      "[Epoch 22 Batch 1000] loss 4.42, perplexity 82.91\n",
      "[Epoch 22 Batch 1500] loss 4.43, perplexity 84.15\n",
      "[Epoch 22] time_cost 46.93s lr 1.000000\n",
      "[Epoch 23 Batch 500] loss 4.41, perplexity 82.47\n",
      "[Epoch 23 Batch 1000] loss 4.41, perplexity 82.56\n",
      "[Epoch 23 Batch 1500] loss 4.43, perplexity 83.58\n",
      "[Epoch 23] time_cost 47.08s lr 1.000000\n",
      "[Epoch 24 Batch 500] loss 4.41, perplexity 82.15\n",
      "[Epoch 24 Batch 1000] loss 4.41, perplexity 82.01\n",
      "[Epoch 24 Batch 1500] loss 4.42, perplexity 83.19\n",
      "[Epoch 24] time_cost 46.83s lr 1.000000\n",
      "[Epoch 25 Batch 500] loss 4.40, perplexity 81.68\n",
      "[Epoch 25 Batch 1000] loss 4.40, perplexity 81.63\n",
      "[Epoch 25 Batch 1500] loss 4.42, perplexity 82.69\n",
      "[Epoch 25] time_cost 46.19s lr 1.000000\n",
      "[Epoch 26 Batch 500] loss 4.40, perplexity 81.39\n",
      "[Epoch 26 Batch 1000] loss 4.40, perplexity 81.33\n",
      "[Epoch 26 Batch 1500] loss 4.41, perplexity 82.32\n",
      "[Epoch 26] time_cost 46.34s lr 1.000000\n",
      "[Epoch 27 Batch 500] loss 4.39, perplexity 80.96\n",
      "[Epoch 27 Batch 1000] loss 4.39, perplexity 80.99\n",
      "[Epoch 27 Batch 1500] loss 4.41, perplexity 81.94\n",
      "[Epoch 27] time_cost 46.76s lr 1.000000\n",
      "[Epoch 28 Batch 500] loss 4.39, perplexity 80.68\n",
      "[Epoch 28 Batch 1000] loss 4.39, perplexity 80.54\n",
      "[Epoch 28 Batch 1500] loss 4.40, perplexity 81.65\n",
      "[Epoch 28] time_cost 46.84s lr 1.000000\n",
      "[Epoch 29 Batch 500] loss 4.39, perplexity 80.29\n",
      "[Epoch 29 Batch 1000] loss 4.39, perplexity 80.29\n",
      "[Epoch 29 Batch 1500] loss 4.40, perplexity 81.36\n",
      "[Epoch 29] time_cost 47.27s lr 1.000000\n",
      "[Epoch 30 Batch 500] loss 4.38, perplexity 80.11\n",
      "[Epoch 30 Batch 1000] loss 4.38, perplexity 79.88\n",
      "[Epoch 30 Batch 1500] loss 4.39, perplexity 81.04\n",
      "[Epoch 30] time_cost 46.46s lr 1.000000\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_params('./rnn_params_epoch_30.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = RNNModel(mode_name, vocab_size, embed_dim, hidden_dim, \n",
    "                 num_layers, dropout_rate)\n",
    "model1.load_params('./rnn_params_epoch_30.txt', ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rnn(model, prefix, num_chars, ctx, idx_to_char, char_to_idx):\n",
    "    prefix = prefix.lower()  # 大写字符转化为小写\n",
    "    hidden = model1.begin_state(func=nd.zeros, \n",
    "                               batch_size=1, ctx=ctx)\n",
    "#     print(hidden[0].shape)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        data = nd.array([output[-1]], dtype='int32', ctx=ctx)\n",
    "        data = data.reshape((1,1))\n",
    "#         print(data.shape)\n",
    "        pred, hidden = model(data, hidden)\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = int(pred.argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 恨别离 不知何处去 不见此时人 不见长安道 何人更见君 送李侍御赴任 刘长卿 一别无人见 孤舟独自归 江山不\n"
     ]
    }
   ],
   "source": [
    "seq = '恨别离'\n",
    "pred_len = 50\n",
    "print('---', predict_rnn(model1, seq, pred_len, ctx, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn.RNN??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = model1.begin_state(func=nd.zeros, \n",
    "                               batch_size=1, ctx=ctx)\n",
    "data = nd.array([char_to_idx['个']], dtype='int32', ctx=ctx)\n",
    "data = data.reshape((1,1))\n",
    "print(data)\n",
    "output, hidden = model1(data, hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(idx_to_char[int(output.argmax(axis=1).asscalar())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
